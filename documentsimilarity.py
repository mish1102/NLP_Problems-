# -*- coding: utf-8 -*-
"""Documentsimilarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYcdntXNgxexVsQ6T-EgJtKDb0qoOPwp
"""

!wget -P /root/input/ -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"

import numpy as np
from gensim.models import KeyedVectors
from gensim.models import KeyedVectors
EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

class DocSim:
    def __init__(self, w2v_model, stopwords=None):
        self.w2v_model = w2v_model
        self.stopwords = stopwords if stopwords is not None else []

    def vectorize(self, doc):
        """Identify the vector values for each word in the given document"""
        doc = doc.lower()
        words = [w for w in doc.split(" ") if w not in self.stopwords]
        word_vecs = []
        for word in words:
            try:
                vec = self.w2v_model[word]
                word_vecs.append(vec)
            except KeyError:
                # Ignore, if the word doesn't exist in the vocabulary
                pass

        # Assuming that document vector is the mean of all the word vectors
        # PS: There are other & better ways to do it.
        vector = np.mean(word_vecs, axis=0)
        return vector

    def _cosine_sim(self, vecA, vecB):
        """Find the cosine similarity distance between two vectors."""
        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))
        if np.isnan(np.sum(csim)):
            return 0
        return csim

    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):
        """Calculates & returns similarity scores between given source document & all
        the target documents."""
        if not target_docs:
            return []

        if isinstance(target_docs, str):
            target_docs = [target_docs]

        source_vec = self.vectorize(source_doc)
        results = []
        for doc in target_docs:
            target_vec = self.vectorize(doc)
            sim_score = self._cosine_sim(source_vec, target_vec)
            if sim_score > threshold:
                results.append({"score": sim_score, "doc": doc})
            # Sort results by score in desc order
            results.sort(key=lambda k: k["score"], reverse=True)

        return results

import re
ds = DocSim(word2vec)
# target_docs = []
# file = open('list_of_sentences.txt','r')
# for row in file:
# 	target_docs.append(row)
# 	target_docs = map(lambda s: s.strip(), target_docs)
# 	target_docs = [x for x in target_docs if x]


target_docs1 = ['good morning','how are you doing ?','the weather is awesome today','samsung','good afternoon','baseball is played in the USA','there is a thunderstorm',
'are you doing good ?',
'The polar regions are melting',
'apple',
'nokia',
'cricket is a fun game',
]

target_docs = [re.sub(r'[^\w\s]', '', _) for _ in target_docs1]

score_dict = {}
expected_output={}

for doc1 in target_docs:
  score_list =[]
  expected_list =[]
  for doc2 in target_docs:
    if(doc1 != doc2):
      score = ds.calculate_similarity(doc1, doc2,threshold=0.55)
      if(len(score)>0):
        score_list.append(score)
        expected_list.append(score[0]['doc'])
  score_dict[doc1]=score_list
  expected_output[doc1]=expected_list
# print(score_dict)
# print(expected_output)
final_list=[]
for k,v in expected_output.items():
  final_list.append(sorted([k,*v]))
result = [list(i) for i in set(map(tuple, final_list))]

!pip install flask-ngrok

from flask_ngrok import run_with_ngrok
from flask import Flask 
from flask import jsonify

app = Flask(__name__)
run_with_ngrok(app)

@app.route('/')
def home():
    # return "Helloo"
    return jsonify(result)
app.run()